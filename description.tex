\documentclass[12pt]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{natbib}

\newcommand\tbd[1]{\textbf{\color{red}(#1)}}

\newcommand\x         {\hbox{$\times$}}
\newcommand\othername {\hbox{$\dots$}}
\def\eq#1{\begin{equation} #1 \end{equation}}
\def\eqarray#1{\begin{eqnarray} #1 \end{eqnarray}}
\def\eqarraylet#1{\begin{mathletters}\begin{eqnarray} #1
                  \end{eqnarray}\end{mathletters}}
\def\mic              {\hbox{$\mu{\rm m}$}}
\def\about            {\hbox{$\sim$}}
\def\Mo               {\hbox{$M_{\odot}$}}
\def\Lo               {\hbox{$L_{\odot}$}}
\def\comm#1           {{\tt (COMMENT: #1)}}
\def\kms   {\hbox{km s$^{-1}$}}
\def\email#1{\href{mailto:#1}{\nolinkurl{#1}}}

\usepackage[usenames]{color}
\newcommand{\G}[1]{{\color{red} #1}}
\newcommand{\B}[1]{{#1}}
\newcommand{\R}[1]{{\color{red}}}

% \code{...} sets code in \tt and escapes most special characters.  Doesn't
% handle {} correctly, but \CODE.anything but period. does (actually, you
% may use any character as a delimiter).
% Designed for a word or two of code inline; use
% \begin{verbatim}...\end{verbatim} for longer blocks
\def\uncatcodespecials{\def\do##1{\catcode`##1=12}\dospecials}%
{\catcode`\`=\active\gdef`{\relax\lq}}% this line inhibits Spanish ligatures ?` and !` of \tt font
\def\setupcode % see TeX Book, p. 381
    {\tt %
     \spaceskip=0pt \xspaceskip=0pt % just in case...
     \catcode`\`=\active
     \uncatcodespecials \obeyspaces
     \catcode`\{=1\catcode`\}=2
    }%
\def\SETUPCODE % see TeX Book, p. 381
    {\tt %
     \spaceskip=0pt \xspaceskip=0pt % just in case...
     \catcode`\`=\active
     \uncatcodespecials \obeyspaces
    }%
\def\docode#1{#1\endgroup}%
\def\DOCODE#1{\def\next##1#1{##1\endgroup}\next}%
\def\code{\begingroup\setupcode\docode}%
\def\CODE{\begingroup\SETUPCODE\DOCODE}%


\title{Hyper Suprime-Cam Survey \\
  Pipeline Description}
\author{
  The HSC Pipeline Team: \\
  Hisanori Furusawa,
  Michitaro Koike,
  Yuki Okura, \\
  Tadafumi Takata,
  Yoshihiko Yamada (NAOJ Mitaka), \\
  Naoki Yasuda,
  Steve Bickerton,
  Sogo Mineo (IPMU), \\
  Robert Lupton,
  Jim Bosch,
  Craig Loomis, \\
  Hironao Miyatake,
  Paul Price (Princeton) \\
}


\begin{document}
\maketitle
\pagestyle{headings}

\begin{abstract}
\end{abstract}

\clearpage

\tableofcontents

\clearpage

\section{Introduction}

This document describes the Hyper Suprime-Cam (HSC) Survey Pipeline, which will be used to process the survey
observations and deliver data products with a consistent high quality to the collaboration, and ultimately to
the world, for the realisation of the survey science goals.

\subsection{Hyper Suprime-Cam}

%%%
%%% Stolen from the SSP proposal, with some edits
%%%

HSC takes advantage of the full accessible field of view of the Subaru telescope (1.5~deg$^2$
diameter).  The focal plane is paved with 104 Hamamatsu Deep Depletion science CCDs (plus another 8 guiding
and wavefront monitoring CCDs), each 2k\x 4k. These chips, which are three-side buttable and each have four
independent readout amplifiers, are currently installed in Suprime-Cam, which has demonstrated their excellent
characteristics: low read noise, excellent charge transfer efficiency, few cosmetic defects, and most
importantly, high quantum efficiency from 4000\AA\ to 10,000\AA.  The CCD pixels are $15\mu$m on a side,
corresponding to $0.16$~arcsec at the focal plane. At this resolution, the images will be well-sampled in even
the best seeing.  Ray-tracing of the optics has shown that ghosting is minimal, with the worst ghost at an
illuminance (fractional light in a PSF aperture) of $\sim 5 \times 10^{-8}$.

Details of the instrument are summarized in the HSC
Design Review
Booklet\footnote{\url{http://anela.mtk.nao.ac.jp/hypersuprime/presentation/hscreview20090227final_combined.pdf}}. The
official HSC
webpage\footnote{\url{http://www.naoj.org/Projects/HSC/index.html}}
provides up-to-date information.

\subsection{The HSC Survey}

%%%
%%% Stolen from the SSP proposal, with edits
%%%

The HSC Survey was initiated to address key scientific questions using the new capabilities afforded by HSC.
The principal science drivers are:
\begin{itemize}
\item Studies of the properties of dark matter and dark energy as a
  function of redshift, using measurements of galaxy clustering, weak
  lensing, supernovae, cross-correlation with high-resolution maps
  of the Cosmic Microwave Background, and studies of high-redshift
  quasars.
\item Studies of the evolution of galaxies over cosmic time, with
  emphasis on stellar populations and star formation, morphologies, and
  active nuclei.
\end{itemize}

The survey has a classical ``wedding cake'' design, with three layers.  The Wide layer will cover roughly
1,400~deg$^2$ in five broad filters ($grizy$), with exposure times of $10-20$~min per pointing (and one with a
shorter exposure time of $\sim 30$~sec to enable astrometric and photometric calibration with other surveys).
This will go to a point-source depth of $r\sim 26$, and is optimized for weak-lensing science.  Using galaxies
for weak lensing will require exquisite understanding of their properties and evolution, which will be the
motivation of the Deep layer, covering 27~deg$^2$ in four distinct fields, going roughly one magnitude deeper
with exposures of roughly 1~hr per filter.  In addition to the broad-band filters, the Deep layer will include
imaging in three narrow-band filters, to detect Lyman-$\alpha$ emitters and study their luminosity function
and clustering properties at $z = 2.2$, 5.7, and 6.6.

To study the faintest and most distant galaxies, and to probe the
transient universe, especially high-redshift supernovae, will require
the Ultradeep layer, going another magnitude fainter yet.  Narrow-band
filters in the Ultradeep layer will search for Lyman-$\alpha$
emitters at $z = 5.7$, 6.6, and 7.3 at the very faint end of the
luminosity function.

More information about the HSC Survey is available from the Strategic Survey Program
proposal\footnote{\url{http://hscsurvey.pbworks.com/Draft-Strategic-Survey-Proposal}} and the HSC Survey White
Paper\footnote{\url{http://hscsurvey.pbworks.com/HSC_Science_White_Paper}}.

\subsection{This document}

This document provides a reference to the HSC Pipeline, which will be
used to process the HSC Survey observations, producing the necessary
data to achieve the scientific goals of the survey.  It is
especially intended for HSC Survey scientists to evaluate the pipeline
and its products in relation to their own science goals.

In section \ref{sec:pipeline}, we outline the pipeline construction and operations, tracing the flow of data
through the various stages of the pipeline, to the ultimate releases to the collaboration.  Next, in section
\ref{sec:products}, we outline the various products that will compose the data releases, with particular
attention to the measurements that comprise the catalog in the database.  Section \ref{sec:algorithms} gives
details as to the algorithms used to make these measurements, and section \ref{sec:interfaces} describes how
the data will be made available.  Finally, section \ref{sec:support} contains pointers on resources available
for supporting users of these data.


\section{The HSC Pipeline}
\label{sec:pipeline}
\subsection{Software}

%%%
%%% Stolen from the SSP proposal
%%%

To construct the HSC pipeline, we use the LSST pipeline software as the foundation.  This is a synergistic
effort, since the pipelines have similar functionality.  By this scheme, the HSC pipeline has features and a
maturity it would not otherwise have at this stage, and because our pipeline is based on code with a larger
market share than ours could ever have alone, we can have greater confidence in the results.  In return, LSST
gains testing and verification with real-world data.  However, since LSST is a larger-scale development
project not entirely within our control, we are selective about the LSST components we use for the HSC
pipeline, so that we are not chained to every design decision of LSST.  We have also supplemented LSST
components with different algorithms, and added new pipeline components as necessary.  The LSST code is
released under the GNU Public License, version 3 (or later), and our releases of our modified versions of that
code, and our own code (as a ``derivative work'') are bound to use the same license.

The pipeline is written in Python, using C++ for low-level algorithmic components where speed is important.
SWIG is used to wrap the C++ components so they can be called directly from Python.  The combination of these
results in all the ease and flexibility of a scripting language while retaining the power of a compiled
language.

Particular attention has been paid, in large part because of our contributions, to making the LSST pipeline
extensible and configurable.  New camera interfaces and new measurement algorithms can be added without the
need to redesign or even rebuild the software.  The modular design also allows for algorithms to be
substituted at run-time, so that the pipeline can adapt to the data being processed.  These features, besides
being useful for our adaptation of the LSST pipeline to HSC, mean that we could use our own pipeline to
process data from other telescopes and cameras for comparison with the results of other pipelines (e.g., we
can not only compare the results of the HSC Survey + HSC pipeline with DECam + DES pipeline, but also with
DECam + HSC pipeline, for better diagnosis of any problems).  Interface packages have been written for LSST
simulated images (``ImSim''), CFHT MegaCam, Subaru's Suprime-Cam, SDSS, the Space Surveillance Telescope
(SST), DECam, and HSC simulated images.

Testing with archival Suprime-Cam data (in particular, from after August 2008 when the Hamamatsu CCDs were
installed) and more recently with HSC commissioning data has been an integral part of the pipeline development.

\subsection{Data flow}

The pipeline consists of multiple stages in order to produce the variety of data products required to meet the
various survey science goals.  The stages share a great deal of framework code, but differ by their inputs,
parallelism scheme (CCD vs exposure vs sky patch) and outputs.  Different science applications will depend on
different pipeline stages, according to the desired measurements, depth and accuracy.  Understanding the data
flow is key to understanding matching science goals with the various measurements provided by the pipeline.

\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.5]{figures/HSCpipelinesketch}
    \caption{HSC Pipeline data flow\label{fig:flow}}
\end{figure}

Figure~\ref{fig:flow} shows a schematic representation of the HSC pipeline.  Input data on the left is fed
into the pipeline stages (light blue, center) to produce the outputs on the right.  The first three stages
(visit processing, mosaic solution, stacking) are executed in sequence, after which all the remaining stages
can be run in parallel.  The stages are explained in more detail below.

\subsubsection{Visit processing}

Visit processing removes the instrumental signature (overscan, bias, dark, flat, bad pixels, amplifier
crosstalk) from each CCD and flags suspected cosmic-rays.  The result is a clean image for which the intensity
is a linear function of astrophysical flux\footnote{Excepting flat-field inaccuracies, and intrinsic pixel
  shape non-linearities.}  The background is estimated and removed, then bright sources on the image are used
to measure the PSF and the astrometric solution and photometric zero-point, before a final detection and
measurement pass.  From the individual CCD measurements, we determine and apply an exposure-wide astrometric
solution and photometric zero-point.

The ouputs are:
\begin{itemize}
\item Image
\item PSF
\item Background
\item Sources
\item Astrometric solution
\item Photometric zero point
\end{itemize}

\subsubsection{Mosaic solution}

Once several exposures overlapping the same part of sky are available, we can determine a mosaic solution.
This involves solving for consistent astrometric solutions and photometric zero-points for the set of
exposures, using sources in common to multiple exposures, in the same manner as \citet{2008ApJ...674.1217P}.
This allows increased accuracy and precision relative to the visit processing, by allowing use of stars
fainter than in our original reference catalog (e.g., SDSS), and incorporating spatial information unavailable
from single exposures.


\subsubsection{Stacking}

Stacking involves the coaddition of multiple images to produce a single, deeper image.  We apply the results
of mosaic solution to the input images, warp them to a common coordinate system (``sky map'';
\S\ref{alg:skymap}) and stack the pixels.  Then we will detect and measure sources on the stack.  Our code
includes a couple of recent innovations to increase the quality of the stacks.

Stacks are normally produced by first subtracting the background, but determining the background from
individual visits separately is problematic (because different choices can be made in each), commonly observed
as dark rings around bright stars and galaxies.  Instead, we perform ``background matching''
(\S\ref{alg:backgroundMatching}) to produce a stack with a high signal-to-noise realisation of the background
in a single reference exposure.  This background can then be measured over wide scales and subtracted with a
high degree of accuracy.

It must always be remembered that a stack is a computational convenience, rather than a perfectly accurate
representation of the sky.  Indeed, if the seeing is not constant with time it becomes impossible to produce a
stack with a PSF varying in a continuous fashion over the field, unless we deliberately degrade the data by
convolving the inputs to a common PSF.  In order to attempt to deal with the discontinuous PSF, we use a
``CoaddPsf'' (\S\ref{alg:coaddPsf}), which is a sum of the PSFs of the input images at each point of interest.

Unfortunately, degrading the PSF is the only way of accurately measuring the colors of galaxies (e.g., for
photometric redshifts) from stacks in the presence of color gradients and seeing changes.  For this reason, we
will produce ``PSF-matched stacks'' for the purpose of measuring galaxy colors.  But because these data have
been deliberately degraded, they are not the deepest available, so we will also produce ``deep stacks'' by
coadding without PSF matching.

Despite the intrinsic limitations of stacks, the measurements from this pipeline stage will be useful for
applications where extreme accuracy isn't required, e.g., searching for sources with extreme spectral features
such as high-$z$ quasars.  The deep stacks will also be used to identify discrete astrophysical objects for
the following stages that require known object positions.

\subsubsection{Forced Stack Measurement}

The deep stack catalog of a nominated band ($i$-band) will be used as a reference to measure sources in
PSF-matched stacks, with the same centroids and apertures.  This provides matched-aperture galaxy colors,
e.g., for photometric redshifts.  Because of the limitations of the stacks, these are not the most accurate
measurements available, but they are more simple and will be available earlier than the full MultiFit
measurements.

\subsubsection{MultiFit}

%%%
%%% Stolen from LSST Data Products document, edited
%%%

The deep stack catalog of a nominated band ($i$-band) will be used as a reference to measure the sources in
multiple bands.  However, rather than coadding a set of images and measuring object characteristics on the
coadd, MultiFit fits a PSF-convolved model simultaneously to multiple independent observations of an
object. This reduces systematic errors, improves the overall signal-to-noise, and allows for fitting of
time-dependent quantities degenerate with shape on the coadds (for example, the proper motion). The models we
plan to fit will not allow for flux variability.  We expect these will be the best possible measurements of
the position, shape and flux of objects of constant flux.

\subsubsection{Forced Visit Measurement}

The deep stack catalog of a nominated band ($i$-band) will be used as a reference to measure the sources in
every available visit.  This produces time-resolved positions (for proper motions and parallax) and fluxes
(for light curves).


\subsubsection{Differencing}

The deep stack images will be used as references to be subtracted from individual visits (or
nightly/monthly/yearly stacks of visits) in order to identify and measure transients.  We will use PSF matching
(\S\ref{alg:psfMatching}) to minimise false positives, but further work will be needed to produce a clean
candidate list.

At the present time, the differencing stage has been greatly neglected in the development of the pipeline, as
more fundamental operations have received priority.  We have prototype code for PSF matching that needs to be
cleaned up, made more robust, and thoroughly tested.  Nevertheless, we are committed to delivering the
differencing stage in advance of the second year of operations for the supernova/transient campaign.


\subsection{Releases}

Data releases are the culmination of the pipeline development and operations, whereby the results are delivered
to the collaboration for science use.

\subsubsection{Daily}

When particular observations have been declared by the transients working group as useful for the
identification of transients (SNe, GRBs), we will attempt to process those observations promptly through a
limited set of the pipeline stages --- visit processing, forced CCD measurements and differencing --- and
deliver the results to the transients working group by 4pm Hawaiian Standard Time the following day.  {\bf It
  should be understood that these releases will be preliminary, and made on a best-efforts basis.}  The
released data will be in the form of FITS images and FITS tables, using the pipeline's internal formats, and
will be placed in a suitable location for bulk download by the transients working group members.

\subsubsection{Monthly}

Aware of the great interest in the HSC survey observations by the collaboration members, and desiring to
encourage independent quality analyses, we will endeavour to produce a preview of the processed data (through
all the pipeline stages) to the collaboration as soon as possible after the observing run.  {\bf It should
be understood that these releases will be preliminary, and made on a best-efforts basis.}  The released data
will be in the form of FITS images for bulk download and a database accessed in the usual manner.

Once the survey is sufficiently progressed that a single month's data is not as interesting, we may not post
the preview, though reducing the data promptly is necessary to inform the observing plan for the following run
of unsuitable exposures in the previous run.


\subsubsection{Semi-annually}

The semi-annual releases are the main goal of the pipeline team.  As part of each, it is our intention to
re-reduce all data from the survey with the best available code and calibrations, delivering all the data
products (\S\ref{sec:products}) to the survey collaboration.  Each semi-annual release will be formally
announced on the HSC mailing list\footnote{\email{hsc@astro.princeton.edu}}, and accompanied by a set of
release notes identifying features and known bugs\footnote{We guarantee that no release will be free of bugs.}
in the release.  The released data may be accessed through the methods described in \S\ref{sec:interfaces}.


\section{Data Products}
\label{sec:products}

Here we describe the various data products produced by the pipeline.

\subsection{Images}

Images from the pipeline shall be multi-extension FITS (MEF) files, containing the image data, mask plane and
variance plane.  The World Coordinate System (WCS) in the headers shall use the \code{TAN-SIP} convention
\footnote{\url{http://fits.gsfc.nasa.gov/registry/sip.html}}, except for those corresponding to a tract/patch
(\S\ref{alg:skymap}) which shall use the standard \code{TAN} projection.  The FITS files may also Contain
further extensions (e.g., for the PSF).  We encourage the use of our software in reading and manipulating the
files.

The mask planes are listed in the mask HDU header (with prefixes \code{MP}).  The symbolic names correspond
to:
\begin{itemize}
\item \code{BAD}: the detector pixel is believed to be bad (e.g., non-functional, uncalibratable, or
  significantly vignetted).
\item \code{SAT}: the pixel was over the full well depth, and therefore likely leaked charge to its neighbours.
\item \code{INTRP}: the pixel value has been derived from interpolating from neighbours, and therefore does
  not properly represent the original value, e.g., if it was originally saturated or flagged as a cosmic ray.
\item \code{CR}: the pixel is believed to be contaminated by a cosmic ray hit.
\item \code{EDGE}: the pixel is close to the edge of the detector, and therefore the source detection threshold will be higher than usual; this has also been used to flag pixels in stacks that have no known value.
\item \code{DETECTED}: the pixel contains a source that has been detected.
\item \code{DETECTED_NEGATIVE}: the pixel contains a negative source that has been detected (usually in the
  difference stage).
\item \code{SUSPECT}: the pixel is non-linear.
\item \code{UNMASKEDNAN}: the pixel contained a \code{NaN} (not-a-number) value that has been suppressed.
\item \code{SYMM_1SIG}, \code{SYMM_3SIG} and \code{MONOTONIC_1SIG}: these are set in the deblender, as an aid
  in determining object shapes.
\end{itemize}

The images are provided principally for the sake of verifying candidates, quality analysis, and to enable
extensive analyses not supported by the pipeline.  If you desire the images in order to generate your own
catalogs (e.g, with SExtractor), please consider using the pipeline catalogs instead, as this will benefit
everybody (e.g., you will have frequent updates with the latest code, and any problems you find can be fed
back into the pipeline).  If the particular measurement you desire to have is not available, please let us
know so that we attempt to support it.

\subsubsection{Visits}

The visit images shall be a single MEF for each CCD, which is background-subtracted and possesses a common
astrometric solution and photometric zero point with the entire visit.  The MEFs shall also include PSF models
for each.  The background model is saved separately, and available if desired.

\subsubsection{Stacks}

The stack images shall be a single MEF for each patch, with a pre-defined astrometric solution and photometric
zero point.  The MEFs shall also include a CoaddPsf (\S\ref{alg:coaddPsf}) for each.

\subsubsection{Differences}

The difference images shall be a single MEF for each patch, with a pre-defined astrometric solution for each,
and a photometric zero point calculated from the original visit.  The MEFs shall also include a suitable PSF
model for each.

\subsection{Catalogs}

The catalogs are the principal data product produced by the pipeline.  It is our goal that most of the science
projects of the survey collaboration can be performed almost entirely from the catalogs; this was the
experience for SDSS.  We encourage collaboration members to discuss with us the addition of particular
measurements or flags that are lacking in the catalogs, so that we can develop and include them.

Catalogs are stored internal to the pipeline as FITS tables.  In these tables, the flags are combined into
a single bit array (FITS format code \code{X}); your software may or may not recognise the flag names in the
header.  In general, it should not be necessary for those outside the pipeline team to read these FITS tables,
as the database (\S\ref{sec:database}) should contain all the same information.

\subsubsection{Visits}

The following measurements will be made as part of the visit processing:
\begin{itemize}
\item Centroid, with error estimate.
\item Flux from PSF photometry, with error estimate.
\item Flux from aperture photometry, with error estimate.
\item Kron flux, with error estimate.
\item Petrosian flux, with error estimate.
\item Gaussian model flux, with error estimate.
\item Adaptive moments.
\item Star/galaxy indicator.
\end{itemize}

\subsubsection{Stacks}

Because we use a CoaddPsf (\S\ref{alg:coaddPsf}), the flags regarding the PSF will not be populated, though
they may be defined.

The following measurements will be made as part of the stack processing:
\begin{itemize}
\item Centroid, with error estimate.
\item Flux from PSF photometry, with error estimate.
\item Flux from aperture photometry, with error estimate.
\item Kron flux, with error estimate.
\item Petrosian flux, with error estimate.
\item Gaussian model flux, with error estimate.
\item Adaptive moments.
\item Shapes from the regaussianization method.
\item Flux from MultiShapelet algorithm, with error estimate.
\item Star/galaxy indicator.
\end{itemize}

\subsubsection{Forced Visit Measurements}

The following measurements will be made as part of the forced visit measurements:
\begin{itemize}
\item Centroid, with error estimate.
\item Flux from PSF photometry, with error estimate.
\item Flux from aperture photometry, with error estimate.
\item Kron flux with reference aperture, with error estimate.
\item Petrosian flux with reference aperture, with error estimate.
\item Gaussian model flux with reference aperture, with error estimate.
\item Star/galaxy indicator.
\end{itemize}

\subsubsection{Forced Stack Measurements}

The following measurements will be made as part of the forced stack measurements:
\begin{itemize}
\item Centroid, with error estimate.
\item Flux from PSF photometry, with error estimate.
\item Flux from aperture photometry, with error estimate.
\item Kron flux with reference aperture, with error estimate.
\item Petrosian flux with reference aperture, with error estimate.
\item Gaussian model flux with reference aperture, with error estimate.
\item Flux from MultiShapelet algorithm with reference aperture, with error estimate.
\item Star/galaxy indicator.
\end{itemize}

\subsubsection{MultiFit}

The following measurements will be made as part of MultiFit:
\begin{itemize}
\item Centroid, with error estimate.
\item Flux from PSF photometry, with error estimate.
\item Flux from aperture photometry, with error estimate.
\item Kron flux, with error estimate.
\item Petrosian flux, with error estimate.
\item Gaussian model, with error estimate.
\item Flux from MultiShapelet algorithm, with error estimate.
\item Star/galaxy indicator.
\end{itemize}


\subsubsection{Differences}

The following measurements will be made as part of differences processing:
\begin{itemize}
\item Centroid, with error estimate.
\item Flux from PSF photometry, with error estimate.
\item Flux from aperture photometry, with error estimate.
\item Kron flux, with error estimate.
\item Petrosian flux, with error estimate.
\item Gaussian model, with error estimate.
\item Star/galaxy indicator.
\end{itemize}


\subsection{Database}
\label{sec:database}
\tbd{Takata-san to provide.}
\subsubsection{Schema}

\section{Algorithms}
\label{sec:algorithms}

Here we describe the various algorithms used within the pipeline.  Of particular interest are the flags
(\S\ref{alg:flags}) and measurements (\S\ref{alg:measurements}) that will comprise the source catalogs
produced in each pipeline stage, and ingested into the database.  Then follows various general algorithms
used in the pipeline.

\subsection{Flags}
\label{alg:flags}

Flags are binary decisions made about each object, and are useful for indicating how a measurement of that
object should be interpreted, and if it is trustworthy.

\subsubsection{Pixel flags}

Pixel flags are flags set on the basis of pixel characteristics.  They comprise:
\begin{itemize}
\item \code{flags.pixel.edge} is set if the source is near the edge of the frame (where the signal-to-noise
  requirement for detection is higher due to the convolution involved; and where measurements may be subject to
  failure).
\item \code{flags.pixel.bad} is set if any pixels in the source footprint have been masked as \code{BAD}
  (typically non-functioning detectors, or severely vignetted areas).
\end{itemize}

The following flags indicate whether the source has been affected by a variety of conditions.  There are two
modes for each: \code{<flag>.any} is set if any pixels in the source footprint have been affected (and
therefore measurements may be impacted only subtly), while \code{<flag>.center} is set if any pixels in the
$3\times 3$ region about the peak have been affected (and therefore measurements may be drastically wrong).

\begin{itemize}
\item \code{flags.pixel.interpolated}: pixels masked as \code{INTERP}, i.e., have been interpolated over, and
  are therefore not the original pixels; this is done for saturated pixels and cosmic rays.
\item \code{flags.pixel.saturated}: pixels masked as \code{SAT}, i.e., are saturated (over full well depth).
\item \code{flags.pixel.suspect}: pixels masked as \code{SUSPECT}, i.e., are non-linear.
\item \code{flags.pixel.cr}: pixels masked as \code{CR}, i.e., suspected cosmic-ray.
\end{itemize}

\subsubsection{Algorithm flags}

Each measurement algorithm (\S\ref{alg:measurements}) has a corresponding algorithm flag that, if set,
indicates the measurement failed; typically this means an exceptional conditional was encountered, such as the
source being too close to the edge.  These algorithm flags are named after the measurement with \code{.flags}
appended, e.g., \code{flux.gaussian.flags}.

Many flux measurement algorithms have an additional flag that may be set if performing the flux measurement on
the PSF at the position of the object failed, and therefore the PSF flux correction cannot be applied (see
\S\ref{alg:measurements}).  These are named after the measurement with \code{.flags.psffactor} appended.

\subsubsection{Calibration flags}

\begin{itemize}
\item \code{calib.detected} is set if the source was detected in calibration (as a bright source).
\item \code{calib.psf.candidate} is set if the source was selected as a candidate PSF star.
\item \code{calib.psf.used} is set if the source was actually used in determining the PSF.
\end{itemize}

\subsubsection{Deblending flags}

The deblender records some details of the deblending through various flags and values in the catalog; see
\S\ref{alg:deblender}.  These allow blended sources to be identified and the family (parents, children,
siblings) to be identified.

\subsubsection{Other}

\begin{itemize}
\item \code{flags.negative} is set if the source was detected as a negative (principally used in the
  difference stage).
\item \code{flags.badcentroid} is set if the principal centroid algorithm (used to feed all other algorithms)
  failed.
\end{itemize}

\subsection{Measurements}
\label{alg:measurements}

The measurement framework we have developed is extensible, so that additional measurement algorithms can
easily be introduced (with only minor additions to the configuration files).  The current measurement
algorithms we use are described below.  The names used refer to the colum names in the catalogs produced by the
pipeline; \tbd{the mapping of these names to the database is not yet known}.

\subsubsection{Centroids}

Each centroiding algorithm provides the centroid (in a column named directly after the algorithm) and a packed
covariance matrix (\code{<algorithm>.err}); note that the covariance term is often \tbd{always?} unset.

\paragraph{centroid.sdss}
The same centroiding algorithm as employed in SDSS \citep{SdssPhoto}, wherein the image is smoothed by the
PSF, and a Gaussian Quartic Interpolation is applied to the pixels surrounding the peak which, in particular,
works for critically-sampled images (whereas a simple parabolic fit does not).  The peak determination is
repeated with the smoothed image binned down, so that a reliable center is produced for extended sources.
David Monet (USNO) reports that this algorithm performs as well as {\sc SExtractor}, and it is our principal
centroider.

\paragraph{centroid.gaussian}
A Gaussian is fit to the source.  Despite originally being developed by David Monet (USNO) for astrometric
use, Monet reports this algorithm is inferior to the SDSS centroid algorithm.

\paragraph{centroid.naive}
A simple, unweighted first moment of the $3\times 3$ pixels around the peak.  This is not a very good centroider (it doesn't use the entire PSF), but is cheap.

\subsubsection{Shapes}

Shape algorithms should provide a shape (typically the 3 second moments; in a column named directly after the
algorithm) and a packed covariance matrix (\code{<algorithm>.err}).  However, because there are different
ways of measuring shapes, there is a diversity of measurements; when a measurement algorithm breaks from the
standard pattern, we detail the outputs.

\paragraph{shape.sdss}
The SDSS adaptive moments, whereby the second moments are measured iteratively, with a Gaussian weight with
size set from the previous iteration.

\paragraph{shape.hsm}
Various shape measurements intended for weak lensing, originally developed by \citet{2003MNRAS.343..459H} and
tested on SDSS by \citet{2005MNRAS.361.1287M}.  The PSF correction methods are:
\begin{itemize}
  \item \code{shape.hsm.ksb}: The KSB \citep{1995ApJ...449..460K} method.
  \item \code{shape.hsm.bj}: The BJ method from \citet{2002AJ....123..583B} Appendix~C.
  \item \code{shape.hsm.linear}: The ``linear'' method, a variation of method (b) described in Appendix B of \citet{2003MNRAS.343..459H}.
  \item \code{shape.hsm.regauss}: The ``re-Gaussianization'' method from Section~2.4 of \citet{2003MNRAS.343..459H} that has been extensively tested \citep{2005MNRAS.361.1287M, 2007MNRAS.376...13M}.
  \item \code{shape.hsm.shapelet}: A ``shapelet'' method, implementing \citet{2003MNRAS.338...35R}.
\end{itemize}
Each of these algorithms provide:
\begin{itemize}
\item \code{<algorithm>.centroid}: The centroid.
\item \code{<algorithm>.moments}: The uncorrected moments.
\item \code{<algorithm>.psf}: Moments of the PSF at the source position.
\item \code{<algorithm>.e1} and \code{<algorithm>.e2}: The ellipticity (\citealt{2002AJ....123..583B};
  \code{nj}, \code{linear}, \code{regauss}) or estimated shear (\code{ksb}, \code{shapelet}).
\item \code{<algorithm>.resolution}: Resolution factor (0 means unresolved, 1 means resolved).
\item \code{<algorithm>.sigma}: Width.
\end{itemize}

\paragraph{shape.miyatake}
A shapelet-based method described in \citet{2012arXiv1209.4643M}, which originates from
\cite{2002AJ....123..583B} and \cite{Nakajima:2006}.  This measurement provides:
\begin{itemize}
\item \code{shape.miyatake.centroid}: The centroid.
\item \code{shape.miyatake.psf}: Moments of the PSF at the source position.
\item \code{shape.miyatake.e1} and \code{shape.miyatake.e2}: Ellipticity.
\item \code{shape.miyatake.e1var}, \code{shape.miyatake.e2var} and \code{shape.miyatake.e1e2cov}: covariance matrix.
\item \code{shape.miyatake.sigmagal} and \code{shape.miyatake.ordergal}: Sigma and order for galaxy GL. \tbd{What is GL?}
\item \code{shape.miyatake.sn}: Signal-to-noise ratio.
\item \code{shape.miyatake.status}: Status code.
\end{itemize}

\subsection{Fluxes}

Each flux measurement algorithm provides a flux (\code{<algorithm>}) and corresponding error
(\code{<algorithm>.err}).

The zero-point of all fluxes is that of the PSF flux, so that fluxes of point sources by each algorithm should
be equal (modulo the quality of the PSF model).  This is done by applying each measurement both to the source
under consideration and the PSF model at that position; each flux measurement therefore has an additional
column in the catalogs beyond those listed below, \code{<algorithm>.psffactor}, which is the flux from
applying the algorithm to the PSF model.

\paragraph{flux.psf}
A PSF flux: the total flux of the source, weighted by the PSF.

\paragraph{flux.sinc}
The total flux of the source within a specified aperture.  This is calculated using sinc interpolation so the
value is the exact amount within the aperture, rather than being scaled to a non-integral number of pixels
after summing only pixels lying entirely within the aperture \citep{2013MNRAS.431.1275B}.

\paragraph{flux.multiShapelet}
Model fluxes are determined from fitting PSF-convolved elliptical models to the source.  We fit separately a
pure exponential model, a pure de\ Vaucouleur model, and a linear combination of these two models (neither
component is allowed to be negative, and the ellipses are fixed, so this is not quite a true bulge-disk
decomposition).  This is similar to the SDSS ``cmodel'' fluxes \citep{SdssPhoto}, except that the profiles are
constructed approximately from sums of Gaussians (8 for each profile; \citealt{HoggLangMoG}), and are
convolved with a shapelet approximation of the PSF.

\paragraph{flux.kron}
Flux using an elliptical aperture defined by \citet{1980ApJS...43..305K}: the Kron radius is measured as the
moment within an (elliptical) aperture of six times the (adaptive) moment, and the Kron flux is measured
within an (elliptical) aperture of twice the Kron radius.  The sinc algorithm is used to measure the exact
flux within this aperture.

\paragraph{flux.petrosian}
Flux using an (elliptical) aperture defined by \citet{1976ApJ...209L...1P} and, implemented in the same manner
as described in Section~A3 of \citet{2002AJ....124.1810S}: the Petrosian radius is the radius, $r$, for which
the surface brightness within an (elliptical) annulus from $0.8r$ to $1.25r$ is 0.2 times the mean surface
brightness within $r$, and the Petrosian flux is measured within an (elliptical) aperture of twice the
Petrosian radius.  \tbd{This measurement is not yet implemented in the pipeline, though prototype code exists
  somewhere.}



\subsection{Background Matching}
\label{alg:backgroundMatching}

This technique will be important for reaching the maximum depth in the Deep and Ultradeep fields.  We use the
algorithm of \citealt{2011arXiv1111.6958H}, extended to two-dimensional data.

The problem is that the common practise of subtracting the background from each input individually removes
features on moderate scales (such as the outer haloes of galaxies, and Galactic cirrus) and can be unstable
(especially when the features appear at the edge of a frame) causing increased noise.  Instead, we choose a
reference exposure, and match the backgrounds of each of the other exposures to the reference.  This is done
by subtracting the reference from each of the other exposures (which mostly removes astrophysical sources,
especially the extended sources which normally contaminate background measurement) and fitting a background
model to the difference between the backgrounds.  These models are then subtracted from the other inputs, so
that all exposures have the same large-scale background as the reference exposure.  After coaddition, the
stack will contain the extended astrophysical features we desire at high signal-to-noise, and the background
can be carefully removed over multiple patches at once.

The subtracted image also provides an opportunity to identify sharp features such as optical ghosts, glints
and other contaminants that can be masked.

\subsection{PSF}

We run separate algorithms to select candidate stars and determine the point-spread function (PSF, the light
distribution for a point source, a critical ingredient to understanding the data and measuring accurate
shapes).  Both the star selector and PSF determiner algorithms are pluggable, so that different algorithms can
be run as desired for different analysis needs.

The ``second-moment'' star selector, for example, builds a histogram of the X and Y second moments of flux,
searches for a peak, and selects sources in the peak as point source candidates.  The ``catalog'' star
selector, in contrast, uses an external catalog of point sources and uses astrometric matching to select point
source candidates.  The ``objectSize'' star selector (our current favourite, as of 2013 August) identifies
point source candidates from the cluster of sources with similar sizes regardless of magnitude.  When
selecting point source candidates by size (i.e., for the ``second-moment'' and ``objectSize'' algorithms), the
sizes are first corrected by the known optical distortion of the camera.

The ``principle-components'' PSF (pcaPsf) determiner, performs a singular value decomposition (also known as a
principal components analysis, or PCA) on the point-source candidates in pixel space to produce a set of
eigen-images.  From a subset of the dominant eigen-images, we construct polynomial interpolants for their
relative weights.  This produces a spatially-varying PSF model that captures the most important changes in the
PSF over the CCD.

We are currently (August 2013) investigating the use of other PSF models, including PSFEx
\citep{2011ASPC..442..435B}.


\subsection{CoaddPsf}
\label{alg:coaddPsf}

Measurements on the stacks will use a PSF constructed from those of the input images (known as ``StackFit'',
pioneered by \citealt{2012arXiv1210.2732J} on the Deep Lens Survey).  In this way, we can accurately follow
the complicated (and discontinuously variable) PSF over the stacked image.  We expect these measurements will
be scientifically useful in their own right, and they will be available relatively quickly, but will not be
the most accurate possible because the stacking process necessarily discards data.

\subsection{Skymap}
\label{alg:skymap}

The ``skymap'' is a tessellation of the sky, providing suitable pre-defined coordinate systems for operations
on the sky such as stacking.  The sky map divides the sky into ``tracts''.  For convenience and parallelism,
each tract is sub-divided into ``patches''.  Tracts and patches overlap, so that sources are not lost in the
gaps.  These overlaps can be both annoying (additional care is required to remove duplicate sources) and
useful (additional source of quality checking).

Our tracts tangent planes (\code{TAN} WCS) distributed in rings of Declination, which matches well with the
survey geometry.  This has more inefficiency (tract overlap) at the pole, but since our survey doesn't cover
that area, it's not a problem.  Individual tracts are roughly the same size as the field of view of HSC, and
have North and East vectors aligned with the columns and rows, respectively.

\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.3]{figures/rings_equat.png}
    \includegraphics[scale=0.3]{figures/rings_pole.png}
    \caption{Demonstration of the skymap scheme we will use for the HSC pipeline.  Left: Equatorial
      projection.  Right: Polar projection.  Note that the tracts shown here are very much larger than we will
      use in practise; the large tracts allows the characteristics of the tessellation (e.g., the larger
      overlap at the poles) to be more clearly discerned.\label{fig:skymap}}
\end{figure}

\subsection{Mosaic}

For each exposure we can calibrate both astrometry and photometry using measurements of objects
matched with reference catalog. But the number of objects are limited by reference catalog and
the we may be affected by some systematic effects. Using multiple exposures and simultaneous fitting
on them enable us more reliable and systematics free calibration. This is known as "uber-calibration" and
applied to SDSS and Pan-STARRS1 photometry. In the pipeline we apply the same kind of method
to each tract. "Uber-calibration" for entire survey region is a separate problem and not yet implemented.

In our meas\_mosaic package we use both catalog matched objects and non-matched objects.
Non-matched objects will be spatially matched between exposures based on astrometry determined 
by chip based analysis. Requirements for non-matched objects having consistent positions/fluxes
contribute to better relative (internal) consistency and requirements for catalog matched objects 
contribute to absolute (external) consistency. 

In astrometry optical distortion of the system is modeled by polynomial functions of focal plane coordinates.
We determine the coefficients of polynomials for each exposure. The locations of CCDs on the focal plane
(offsets and rotations) are also fitting parameters.
In photometry correction for the flat-field is modeled by polynomial functions of focal plane coordinates.
Same as astrometry, the coefficients of polynomials are determined. Relative flux scales between exposures
are also determined.

The mathematical functional forms are the following.

In the following, super(sub)scription $s$, $e$, and $c$ denotes stars, exposures and chips, respectively.
$(x,y)$ is a pixel coordinate within a chip, $(u,v)$ is a focal plane coordinate, $(\xi,\eta)$ is an projected
celestial coordinate of the sky at $(A,D)$.

For astrometry, we solve the equation for $\Delta a_k^e$, $\Delta b_k^e$, $\Delta X_c$, $\Delta Y_c$, and
$\Delta\theta_c$ until it converges. For non-matched sources $\Delta\alpha^s$ and $\Delta\delta^s$ are also
solved.  For matched sources their catalog values are used for their $(\alpha, \delta)$ and they are fixed.
$\chi^2$ equation is linearized at eq (5). Initial values are estimated from the fit to matched sources for
individual exposures. Only multiply ($n\ge2$) observed non-matched sources are used in the fitting.

\footnotesize

\begin{equation}
\left(
\begin{array}{c}
u^{s,e} \\
v^{s,e} 
\end{array}
\right)
=
\left(
\begin{array}{cc}
\cos \theta_c & -\sin \theta_c \\
\sin \theta_c & \cos \theta_c
\end{array}
\right)
\left(
\begin{array}{c}
x^{s,e,c} \\
y^{s,e,c} 
\end{array}
\right)
+
\left(
\begin{array}{c}
X_c \\
Y_c
\end{array}
\right)
=
\left(
\begin{array}{c}
x^{s,e,c}\cos\theta_c-y^{s,e}\sin\theta_c+X_c \\
x^{s,e,c}\sin\theta_c+y^{s,e}\cos\theta_c+Y_c
\end{array}
\right)
\nonumber
\end{equation}

\begin{equation}
\left(
\begin{array}{c}
\xi^{s,e} \\
\eta^{s,e}
\end{array}
\right)
=
\left(
\begin{array}{c}
\xi(\alpha^{s}, \delta^{s}, A^{e}, D^{e}) \\
\eta(\alpha^{s}, \delta^{s}, A^{e}, D^{e})
\end{array}
\right)
\end{equation}

\begin{eqnarray}
\chi^2 & = & \sum_e \sum_s \left\{ \xi^{s,e}-\sum_k a_k^e [u^{s,e}]^{i(k)} [v^{s,e}]^{j(k)} \right\}^2 + \sum \left\{ \eta^{s,e}-\sum_k b_k^e [u^{s,e}]^{i(k)} [v^{s,e}]^{j(k)} \right\}^2 \\
& = & \sum_e \sum_s \left\{ \xi^{s,e}-\sum_k a_k^e [x^{s,e}\cos\theta_c-y^{s,c}\sin\theta_c+X_c]^{i(k)} [x^{s,e}\sin\theta_c+y^{s,c}\cos\theta_c+Y_c]^{j(k)} \right\}^2 \nonumber \\
& + & \sum_e \sum_s \left\{ \eta^{s,e}-\sum_k b_k^e [x^{s,e}\cos\theta_c-y^{s,c}\sin\theta_c+X_c]^{i(k)} [x^{s,e}\sin\theta_c+y^{s,c}\cos\theta_c+Y_c]^{j(k)} \right\}^2 \\
& = & \sum_e \sum_s \Biggl\{ \xi^{s,e} + \frac{\partial \xi^{s,e}}{\partial \alpha^{s}} \Delta\alpha^{s} + \frac{\partial \xi^{s,e}}{\partial \delta^{s}} \Delta\delta^{s}  \nonumber \\
&  & \hspace{0.7cm}- \left(\sum_k a_k^e [u^{s,e}]^{i(k)} [v^{s,e}]^{j(k)} \right. +\sum_k \Delta a_k^e [u^{s,e}]^{i(k)} [v^{s,e}]^{j(k)} \nonumber \\
&  & \hspace{1.0cm}+ \sum_k a_k^e \cdot i(k) \cdot [u^{s,e}]^{i(k)-1} [v^{s,e}]^{j(k)} \Delta X_c + \sum_k a_k^e \cdot j(k) \cdot [u^{s,e}]^{i(k)} [v^{s,e}]^{j(k)-1} \Delta Y_c \nonumber \\
&  & \hspace{1.0cm}+ \sum_k \Biggr[ a_k^e [u^{s,e}]^{i(k)-1} [v^{s,e}]^{j(k)-1}  \nonumber \\
& & \hspace{1.0cm} \{-i(k)[v^{s,e}](x^{s,e}\sin\theta_c+y^{s,e}\cos\theta_c)+j(k)[u^{s,e}](x^{s,e}\cos\theta_c-y^{s,e}\sin\theta_c) \} \Biggr] \Delta \theta_c \Biggr) \Biggr\}^2 \nonumber \\
& + & \sum_e \sum_s \Biggl\{ \eta^{s,e} + \frac{\partial \eta^{s,e}}{\partial \alpha^{s}} \Delta\alpha^{s} + \frac{\partial \eta^{s,e}}{\partial \delta^{s}} \Delta\delta^{s}  \nonumber \\
&  & \hspace{0.7cm}- \left(\sum_k b_k^e [u^{s,e}]^{i(k)} [v^{s,e}]^{j(k)} \right. +\sum_k \Delta b_k^e [u^{s,e}]^{i(k)} [v^{s,e}]^{j(k)} \nonumber \\
&  & \hspace{1.0cm}+ \sum_k b_k^e \cdot i(k) \cdot [u^{s,e}]^{i(k)-1} [v^{s,e}]^{j(k)} \Delta X_c + \sum_k b_k^e \cdot j(k) \cdot [u^{s,e}]^{i(k)} [v^{s,e}]^{j(k)-1} \Delta Y_c \nonumber \\
&  & \hspace{1.0cm}+ \sum_k \Biggr[ b_k^e [u^{s,e}]^{i(k)-1} [v^{s,e}]^{j(k)-1}  \nonumber \\
& & \hspace{1.0cm} \{-i(k)[v^{s,e}](x^{s,e}\sin\theta_c+y^{s,e}\cos\theta_c)+j(k)[u^{s,e}](x^{s,e}\cos\theta_c-y^{s,e}\sin\theta_c) \} \Biggl] \Delta \theta_c \Biggr) \Biggr\}^2 \\
& = & \sum_e \sum_s \left\{ A_x^{s,e} - \sum_k \Delta a_k^e [u^{s,e}]^{i(k)} [v^{s,e}]^{j(k)} - B_x^{s,e} \Delta X_c - C_x^{s,e} \Delta Y_c -D_x^{s,e} \Delta\theta_c + \frac{\partial \xi^{s,e}}{\partial \alpha^{s}} \Delta\alpha^{s} + \frac{\partial \xi^{s,e}}{\partial \delta^{s}} \Delta\delta^{s} \right\}^2 \nonumber \\
& + & \sum_e \sum_s \left\{ A_y^{s,e} - \sum_k \Delta b_k^e [u^{s,e}]^{i(k)} [v^{s,e}]^{j(k)} - B_y^{s,e} \Delta X_c - C_y^{s,e} \Delta Y_c -D_y^{s,e} \Delta\theta_c + \frac{\partial \eta^{s,e}}{\partial \alpha^{s}} \Delta\alpha^{s} + \frac{\partial \eta^{s,e}}{\partial \delta^{s}} \Delta\delta^{s} \right\}^2 \nonumber \\
\end{eqnarray}

\begin{equation}
A_x = \xi - \sum_k a_k [u]^{i(k)} [v]^{j(k)}
\end{equation}

\begin{equation}
A_y = \eta - \sum_k b_k [u]^{i(k)} [v]^{j(k)}
\end{equation}

\begin{equation}
B_x = \sum_k a_k \cdot i(k) \cdot [u]^{i(k)-1} [v]^{j(k)}
\end{equation}

\begin{equation}
B_y = \sum_k b_k \cdot i(k) \cdot [u]^{i(k)-1} [v]^{j(k)}
\end{equation}

\begin{equation}
C_x = \sum_k a_k \cdot j(k) \cdot [u]^{i(k)} [v]^{j(k)-1}
\end{equation}

\begin{equation}
C_y = \sum_k b_k \cdot j(k) \cdot [u]^{i(k)} [v]^{j(k)-1}
\end{equation}

\begin{equation}
D_x = \sum_k a_k [u]^{i(k)-1} [v]^{j(k)-1} \{-i(k)[v](x^s\sin\theta_c+y^s\cos\theta_c)+j(k)[u](x^s\cos\theta_c-y^s\sin\theta_c) \} 
\end{equation}

\begin{equation}
D_y = \sum_k b_k [u]^{i(k)-1} [v]^{j(k)-1} \{-i(k)[v](x^s\sin\theta_c+y^s\cos\theta_c)+j(k)[u](x^s\cos\theta_c-y^s\sin\theta_c) \} 
\end{equation}

For photometry, we solve for $f_{i,j}$, $dm^e$, and $m_0^s$. $m_0^s$ is the true magnitudes of stars and 
for catalog matched objects $m_0^s$ will be replaced with $m_{cat}^s$ and fixed to give absolute
calibration.

\begin{equation}
\chi^2 =  \sum_e \sum_s \left\{ m_0^s - \Biggl( m^{s,e}+dm^e+\sum_{i+j\le n} f_{i,j} [u^{s,e}]^i [v^{s,e}]^j \Biggr) \right\}^2
\end{equation}

\normalsize

\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.3]{figures/posScatter150.png}
    \includegraphics[scale=0.3]{figures/fluxScatter150.png}
    \caption{Results of mosaic solution for DITH-16H large dither
      dataset (4x4 visits). In scatter plot, green points are external consistency
      adn red points are internal consistency. In fluxScatter blue points are internal
      consistency only for bright objects.\label{fig:mosaic}}
\end{figure}



\subsection{PSF matching}
\label{alg:psfMatching}

PSF matching allows us to match the PSF of an input image to that of a target model or target image.  The
images with matched PSFs can be subtracted (to find transients) or coadded (to produce a stack with a PSF that
varies in a continuous manner over the image).  We use the algorithm of \citet{1998ApJ...503..325A} and
\citet{2000A&AS..144..363A}.  We have the option of using the traditional \citet{1998ApJ...503..325A}
Gaussian/polynomial basis functions, or regularised delta-function basis functions
\citep{2012MNRAS.425.1341B}.


\subsection{Deblending}
\label{alg:deblender}

At the depths probed by HSC images, many of the sources are superimposed on each other on the sky
(``blended''), which makes detecting and measuring such sources difficult.  Often this blending is not severe,
but the $5\sigma$ contours overlap (and therefore merge together) slightly.  In other cases, distinct objects
will be superimposed directly on more extended objects (e.g., a star on a resolved galaxy).  In order to
disentangle the multiple objects, we employ a ``deblender'' based on the experience of SDSS
\citep{2005ASPC..338..151L}.

The deblender assumes that discrete sources generally have twofold
rotational symmetry.  When one side of a source is blended, we can
recover its appearance by examining the symmetric side.  The deblender
begins by building these symmetric templates for each source.  Next,
the flux in each pixel is split among the blended source in proportion
to their templates.  The deblender produces ``postage stamps'' of each
source in a blended group, so that the measurement algorithms (fluxes,
galaxy shapes, etc) need not know that the source was blended.

This deblending algorithm works well in practise for moderately
crowded fields (as demonstrated by SDSS).

The deblender (\S\ref{alg:deblender}) sets the following flags based on its interpretation of the source:
\begin{itemize}
\item \code{deblend.deblended-as-psf} is set if the deblender considered the source as a PSF.
\item \code{deblend.too-many-peaks} is set if the deblender attempted to divide the source beyond a limit;
  only the brightest peaks were deblended.
\item \code{deblend.failed} is set if deblending failed on this source.
\end{itemize}

And the following values are set:
\begin{itemize}
\item \code{parent}: if non-zero, the \code{id} of the source from which this source was deblended.
\item \code{deblend.nchild}: the number of children this source was deblended into.
\item \code{deblend.psf-center}: the centroid of the PSF used, if \code{deblended-as-psf}.
\item \code{deblend.psf-flux}: the flux of the PSF used, if \code{deblended-as-psf}.
\end{itemize}

\subsection{Star/galaxy separation}

\subsection{Cosmic ray identification}

We use the same algorithm as in SDSS to identify cosmic-rays by their morphology (see Section 4.3 of
\citealt{SdssPhoto}).  We have found this works well on the Hamamatsu thick CCDs in Suprime-Cam.

\subsection{Fringe subtraction}

The Suprime-Cam data shows mild fringing in the $y$-band.  We construct fringes\footnote{This code fell out a
  little while ago, but it is easy to revive it.} by combining with rejection many night-time science
exposures.  To apply, the mean values are measured in a large number (e.g., 30,000) of randomly positioned
small regions (e.g., 10 pixels square), both on the fringe frame and the science frame, from which the relative
scaling of the fringe frame may be determined (with careful clipping to discard regions affected by
astrophysical sources).  The scaled fringe frame is then subtracted from the science frame.

\subsection{Background models}

A smooth background is estimated and removed from the image.  We begin by measuring the background level in
cells (typically 256 or 512 pixels square) using (by default) a clipped mean, and ignoring pixels that are
part of detected sources.  We then construct an interpolating spline (Akima spline, by default) to estimate
the background level in each pixel.

\subsection{Warping}

In warping an image from the detector frame to the sky frame, we apply a resampling kernel.  The kernel is set
according to the sub-pixel position on the input image of the centre of the corresponding output pixel.  The
code supports using Lanczos (of configurable order), bilinear or nearest-neighbour kernels; we use a 3rd-order
Lanczos (with $10^6$ cache realisations), as a compromise between the infinite Sinc function and the need for
speed.


\section{External Interfaces}
\label{sec:interfaces}
\tbd{Koike-san to provide.}
\subsection{Database}
\subsection{Postage Stamps}
\subsection{Copying}

\section{Help and Support}
\label{sec:support}

The pipeline team is committed to supporting the collaboration in their understanding of the pipeline and the
use of its products.  To this end, there are two principal means of interacting with the pipeline team.  We
encourage collaboration members to use these means, rather than asking pipeline team members individually,
since they allow us to build up a body of support documentation that will be of use to the entire
collaboration.  In contrast, questions asked and answered privately do not benefit anyone else.

\subsection{Forum}

A web-based question-answer forum, similar to that of the popular coding forum \url{stackoverflow.com} has
been set up at \url{http://hsca.ipmu.jp:88/questions/}.  Collaboration members are encouraged to set up an
account and ask (and answer!) questions and vote on questions and answers.  Members of the pipeline team
receive e-mail notifications of postings, so that questions from collaboration members can be answered
promptly.  When you receive a suitable answer to your question, please ``accept'' it (by clicking on the check
mark to the left of the answer), so that everyone else can identify the correct solution.

\subsection{Mailing list}

The pipeline team (and interested bystanders) can be reached by e-mail to
\email{hsc_software@astro.princeton.edu}.  Collaboration members are welcome to sign up at
\url{http://jeeves.astro.princeton.edu/mailman/listinfo/hsc_software}.  The list archive is publicly
accessible, at \url{http://jeeves.astro.princeton.edu/pipermail/hsc_software/}


\clearpage

\bibliographystyle{}
\bibliography{pipeline/pipeline}

\end{document}
